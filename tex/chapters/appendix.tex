\documentclass[../main.tex]{subfiles}

\begin{document}
\chapter{Weyl Spinors \& Grassmann Calculus}
\label{app:chap:weyl_grassmann}
Here, I present some general properties of Weyl spinors, and define what is meant by derivatives and integration on Grassmann-valued spaces.
These definitions were in \cref{chap:susy}, particularly in defining superspace and working out Feynman rules from the superlagrangian.

\section{Weyl Spinors}
Weyl spinors are two-component vectors in a representation space of \(SL(2,\mathbb{C})\).
There are representations of \(SL(2,\mathbb{C})\) that form Weyl spinor spaces -- the self representation and its complex conjugate.
Spinors in the self-representation space of \(SL(2, \mathbb{C})\) are referred to as \emph{left-handed} spinors, and spinors in the conjugate representation space as \emph{right-handed}.
Each of these vector spaces has a dual-space, which we keep track of by raising and lowering indices.
Given a Weyl spinor \(\psi\) with components \(\psi\_\alpha\) in the self-representation space, we can raise its indices using the antisymmetric tensor
\begin{equation}
  \epsilon\^{\alpha\beta} = \begin{pmatrix}
    0 & 1 \\ -1 & 0
  \end{pmatrix}.
\end{equation}
To lower the indices, we can use
\begin{equation}
  \epsilon\_{\alpha\beta} = \begin{pmatrix}
    0 & -1 \\ 1 & 0
  \end{pmatrix}.
\end{equation}
We can define a vector \(\psi^\dagger\) in the conjugate representation by taking the complex conjugate of a vector in the self-representation:
\begin{equation}
  \psi_{\dot\alpha}^\dagger = (\psi\_\alpha)^\dagger,
\end{equation}
where I use dotted indices to indicate components of a vector in the conjugate representation.
Contractions of two Weyl spinors \(\psi\) and \(\phi\) is defined by
\begin{subequations}
  \begin{eqnarray}
    (\psi\phi) &\equiv& \psi\^\alpha \phi\_\alpha = \epsilon\^{\alpha\beta} \psi\_\beta \phi\_\alpha = \psi\_2 \phi\_1 - \psi\_1 \phi\_2, \\
    (\psi\phi)^\dagger &\equiv& \psi^\dagger_{\dot\alpha} \phi^{\dagger \dot \alpha} = \epsilon\^{\dot\alpha \dot\beta} \psi^\dagger_{\dot\alpha} \phi^\dagger_{\dot\beta} = \psi^\dagger_1 \phi^\dagger_2 - \psi^\dagger_2 \phi^\dagger_1.
  \end{eqnarray}
\end{subequations}
\medskip

For Grassmann-valued Weyl spinors, the components anti-commute.
This implies
\begin{equation}
  \psi^2 \equiv (\psi\psi) = -\psi_{1}\psi_{2}.
\end{equation}
Some useful relations for Grassmann-valued Weyl spinors \(\psi, \eta, \phi\) are
\begin{subequations}
  \begin{eqnarray}
    \eta\psi & = & \psi\eta, \\
    \bar{\eta}\bar{\psi} & = & \bar{\psi}\bar{\eta}, \\
    \pclosed{\eta\psi}^\dagger & = & \bar{\psi}\bar{\eta}, \\
    \pclosed{\eta\psi}\pclosed{\eta\phi} & = & -\frac{1}{2} \pclosed{\eta\eta}\pclosed{\psi\phi}, \\
    \eta\sigma^\mu\bar{\psi} & = & -\bar{\psi} \bar{\sigma}^\mu \eta, \\
    (\sigma^\mu \bar{\eta})_{\alpha} (\eta\sigma^\nu\bar{\eta}) & = & \frac{1}{2} g^{\mu\nu} \eta_{\alpha} (\bar{\eta}\bar{\eta}), \\
    (\eta\sigma^\mu\bar{\eta})(\eta\sigma^\nu\bar{\eta}) & = & \frac{1}{2} g^{\mu\nu} (\eta\eta)(\bar{\eta}\bar{\eta}), \\
    (\eta\sigma^\mu \partial_\mu \bar{\psi}) (\eta\psi) & = & -\frac{1}{2} (\psi\sigma^\mu \partial_\mu \bar{\psi})(\eta\eta), \\
    (\partial_\mu \sigma^\mu \bar{\eta}) (\bar{\eta}\bar{\psi}) &= & -\frac{1}{2} (\partial_\mu \psi \sigma^\mu \bar{\psi}) (\bar{\eta}\bar{\eta}), \\
    (\bar\eta\bar\psi) (\eta\sigma^\mu\bar\eta) (\eta\psi) & = & \frac{1}{4} (\eta\eta) (\bar\eta\bar\eta) (\psi\sigma^\mu\bar\psi), \\
    \eta \sigma^{\mu\nu} \psi & = & -\psi \sigma^{\mu\nu} \eta,
  \end{eqnarray}
\end{subequations}
where \(\sigma\^\mu = \pclosed{\eye, \sigma\^i}\), \(\bar\sigma\^\mu = \pclosed{\eye,-\sigma\^i}\), \(\sigma\^{\mu\nu} = \frac{i}{4} \pclosed{\sigma\^\mu \bar\sigma\^\nu - \sigma\^\nu \bar\sigma\^\mu}\), and \(\sigma\^i\) are the Pauli matrices given by
\begin{subequations}
  \begin{eqnarray}
    \sigma^1 &=& \begin{pmatrix}
      0 & 1 \\ 1 & 0
    \end{pmatrix}, \\
    \sigma^2 &=& \begin{pmatrix}
      0 & -i \\ i & 0
    \end{pmatrix}, \\
    \sigma^3 &=& \begin{pmatrix}
      1 & 0 \\ 0 & -1
    \end{pmatrix}.
  \end{eqnarray}
\end{subequations}

\subsection*{Dirac Spinors}
We can construct Dirac spinors from the Weyl spinors by stacking a left- and a right-handed Weyl spinor into a four-component spinor.
Given a two left-handed Weyl-spinors \(\psi_L\) and \(\psi_R\), we can construct a Dirac spinor \(\Psi\) as
\begin{equation}
  \Psi = \begin{pmatrix}
    \psi_L \\ \psi_R^\dagger
  \end{pmatrix},
\end{equation}
with a conjugate spinor defined as
\begin{equation}
  \bar\Psi = \Psi^\dagger \gamma\^0 = \begin{pmatrix}
    \psi_R \\ \psi_L^\dagger
  \end{pmatrix}.
\end{equation}
Here I have used the zeroth gamma matrix.
For reference, in the Weyl representation of Dirac spinors used here these are defined as
\begin{equation}
  \gamma\^\mu = \begin{pmatrix}
    0 & \sigma\^\mu \\ \bar\sigma\^\mu & 0
  \end{pmatrix}.
\end{equation}
\medskip

A basis for operators on Dirac spinors space is
\begin{equation}
  \Gamma\^r = \cclosed{\eye, \gamma\^5, \gamma\^\mu, \gamma\^\mu\gamma\^5, \gamma\^{\mu\nu}},
\end{equation}
where \(\gamma\^5 = i\gamma\^0\gamma\^1\gamma\^2\gamma\^3\) and
\begin{equation}
  \gamma\^{\mu\nu} = \frac{i}{2} \pclosed{\gamma\^\mu \gamma\^\nu - \gamma\^\nu \gamma\^\mu}.
\end{equation}
I also give the charge conjugation matrices, defined with the properties
\begin{subequations}
  \begin{eqnarray}
    C^{-1} &=& C^\dagger, \\
    C^T &=& -C,
  \end{eqnarray}
\end{subequations}
and work on an arbitrary operator on Dirac spinor space in the following way
\begin{equation}
  C \Gamma_r^T C^{-1} = \eta_r \Gamma_r, \qquad \eta_r = \begin{cases}
    1  & \text{for }\eye, \gamma\^5, \gamma\_\mu \gamma\^5 \\
    -1 & \text{for }\gamma\_\mu, \gamma\_{\mu\nu}
  \end{cases}.
\end{equation}


\section{Grassmann Calculus}
Here I present the calculus of two-component Grassmann valued Weyl spinors \(\theta\_\alpha\).
Grassmann number are defined such that they anticommute.
For two Grassmann number \(\theta\) and \(\eta\) we have
\begin{equation}
  \theta \eta = -\eta \theta,
\end{equation}
implying \(\theta^2 = \eta^2 = 0\).
This in turn means that a function \(f(\theta)\) can be expanded in full as
\begin{equation}
  f(\theta) = a_0 + a_1 \theta,
\end{equation}
for some coefficient real-valued (or potential complex-valued) coefficients \(a_0, a_1\).
This is seen by a Taylor expansion of \(f\) around \(\theta = 0\).\footnote{0 is trivially a Grassmann number too.}

Integration is defined a kind of projection operator, defining
\begin{equation}
  \integral{\theta} \equiv 0, \qquad \integral{\theta} \theta \equiv 1,
\end{equation}
we have
\begin{subequations}
  \begin{eqnarray}
    \integral{\theta} f(\theta) &=& a_1, \\
    \integral{\theta} \theta f(\theta) &=& a_0.
  \end{eqnarray}
\end{subequations}
If we define derivation with respect to a Grassmann variable \(\theta\) as
\begin{equation}
  \d[\theta]\eta \equiv 0,\qquad \d[\theta] \theta \equiv 1,
\end{equation}
we get that it will work similarly to integration in that
\begin{equation}
  \d[\theta]\ f(\theta) = a_1 = \integral{\theta} f(\theta).
\end{equation}
\medskip

Expanding this to the four Grassmann variable in superspace, \(\theta\_\alpha, \theta^\dagger_{\dot\alpha}\), we define the integration measures
\begin{subequations}
  \begin{eqnarray}
    \mathrm{d}^2\theta &\equiv& -\frac{1}{4} \mathrm{d}\theta\^\alpha \mathrm{d}\theta\_\alpha, \\
    \mathrm{d}^2\theta^\dagger &\equiv& -\frac{1}{4} \mathrm{d}\theta^\dagger_{\dot\alpha} \mathrm{d}\theta^{\dagger \dot\alpha}, \\
    \mathrm{d}^4\theta &\equiv& \mathrm{d}^2\theta \mathrm{d}^2\theta^\dagger,
  \end{eqnarray}
\end{subequations}
such that we have
\begin{subequations}
  \begin{eqnarray}
    \integral{^2\theta} (\theta\theta) &=& 1, \\
    \integral{^2\theta^\dagger} (\theta\theta)^\dagger &=& 1, \\
    \integral{^4\theta} (\theta\theta)(\theta\theta)^\dagger &=& 1.
  \end{eqnarray}
\end{subequations}


\chapter{Takagi Factorisation Algorithm}
\label{app:chap:takagi}
% \section{Schur decomposition and singular-value decomposition}
% Schur decomposition tells us that any (potentially complex) matrix \(A\) can be written as
% \[
%   A = U^\dagger \Delta U,
% \]
% where \(U\) is a unitary matrix, and \(\Delta\) is an upper triangular matrix.
% It follows then that if \(A\) is a symmetric matrix (\(A^T = A\)), then
% \[
%   \pclosed{U^\dagger \Delta U} = \pclosed{U^\dagger \Delta U}^T = U^T \Delta^T U^\ast
% \]

Here, I go through the proofs necessary for the procedure defined in \cref{susy:sec:takagi} to find the Takagi diagonalising matrix \(U\) s.t. for a complex, symmetric matrix
\begin{equation}
  A = U^T A U.
\end{equation}

\section{Proofs}
\paragraph{The Takagi vector.}
For any \(A \in M_{n}(\mathbb{C})\) such that \(AA^\ast\) only has real, non-negative eigenvalues, there exists a non-zero vector \(\vec{v} \in \mathbb{C}^n\) such that \(A \vec{v}^* = \sigma \vec{v}\), where \(\sigma\) is a real, non-negative number.\\
\emph{Proof.} Consider a vector \(\vec{x} \neq \vec{0} \in \mathbb{C}^n\) that is an eigenvector of \(AA^\ast\) with corresponding eigenvalue \(\lambda\).
There are two cases:
\begin{itemize}
  \item[(a)] \(A\vec{x}^*\) and \(\vec{x}\) are linearly dependent.
  \item[(b)] \(A\vec{x}^*\) and \(\vec{x}\) are linearly independent.
\end{itemize}
In case (a), we must have that \(A\vec{x}^* = \mu \vec{x}\) for some \(\mu \in \mathbb{C}\), since they are linearly dependent.
Then \(AA^\ast \vec{x} = A \mu^\ast \vec{x}^* = \abs{\mu}^2 \vec{x} \equiv \lambda \vec{x}\), which is non-negative by definition.\\
In case (b), the vector \(\vec{y} = A\vec{x}^* + \mu \vec{x}\) is non-zero for any \(\mu \in \mathbb{C}\), since \(A\vec{x}^*\) and \(\vec{x}\) are linearly independent.
Then we can choose \(\mu\) such that \(\abs{\mu}^2 = \lambda\) to get that \(A \vec{y}^* = A\pclosed{A^\ast \vec{x} + \mu^\ast \vec{x}^*} = \lambda\vec{x} + \mu^\ast A\vec{x^*} = \mu\mu^\ast \vec{x} + \mu^\ast A\vec{x}^* = \mu^\ast \pclosed{A \vec{x}^* + \mu \vec{x}} = \mu^\ast \vec{y}\).\\
As such, we can always find a vector \(\tilde{\vec{v}} \in \mathbb{C}^n\) such that \(A \tilde{\vec{v}}^* = \mu \tilde{\vec{v}}\) for some \(\mu \in \mathbb{C}^n\).
Furthermore, we can define a vector \(\vec{v} = \exp{i\theta} \tilde{\vec{v}}\) for a \(\theta \in \mathbb{R}\) to get \(A\vec{v}^* = A \pclosed{\exp{i\theta} \tilde{\vec{v}}}^\ast = \exp{-i\theta} A\tilde{\vec{v}}^* = \exp{-i\theta} \mu \tilde{\vec{v}} = \exp{-2i\theta} \mu \exp{i\theta} \tilde{\vec{v}} = \exp{-2i\theta} \mu \vec{v} \equiv \sigma \vec{v}\).
This allows us to choose the phase of \(\sigma = \exp{-2i\theta} \mu\) to be such that \(\sigma\) is real and non-negative.
\medskip

\paragraph{Eigenvalues of \(AA^\ast\) for symmetric \(A\).}
Given an \(N \by N\) complex matrix \(A\), the eigenvalues of \(AA^\ast\) are always real and non-negative.\\
\emph{Proof}.
Consider \(\vec{x} \neq \vec{0}\) an eigenvector of \(AA^\ast\) with corresponding eigenvalue \(\lambda\).
Then we must have that
\[\lambda \vec{x}^\dagger \vec{x} = \vec{x}^\dagger AA^\ast \vec{x} = \pclosed{A^\dagger \vec{x}}^\dagger \pclosed{A^\ast \vec{x}} = \pclosed{A^\ast \vec{x}}^\dagger \pclosed{A^\ast \vec{x}},\]
where we have used that \(A^\dagger = \pclosed{A^T}^\ast = A^\ast\).
This means that \(\lambda \geq 0\), since for any vector \(\vec{v} \in \mathbb{C}^n\) we have that \(\vec{v}^\dagger \vec{v} \geq 0\).
As this holds for all eigenvectors \(\vec{x}\) of \(AA^\ast\), all its eigenvalues must be non-negative.

\paragraph{Diagonalisation step of a symmetric matrix \(A\).}
For any symmetric matrix \(A \in M_n(\mathbb{C})\), there exist a unitary matrix \(V \in M_n(\mathbb{C})\) such that
\[V^\dagger A V^\ast = \begin{pmatrix} \sigma & \vec{0} \\ \vec{0} & A_2 \end{pmatrix},\]
where \(\sigma\) is a real, non-negative number and \(A_2 \in M_{n-1}(\mathbb{C})\) is also a symmetric matrix. \\
\emph{Proof.}
Consider a normalised Takagi vector \(\vec{v} \neq \vec{0}\) of \(A\) such that \(A\vec{v}^\ast = \sigma \vec{v}\) for some real, non-negative \(\sigma\) and \(\vec{v}^\dagger \vec{v} = 1\).
We can then complete an orthonormal basis for \(\mathbb{C}^n\) with unit vectors \(\vec{v}_i\) where \(i \in 1, \ldots, n\), where we define \(\vec{v}_1 \equiv \vec{v}\).
Defining a unitary matrix \(V = \bclosed{\vec{v}_1, \ldots, \vec{v}_{n}}\), the first column of the product
\[\pclosed{V^\dagger A V^\ast}_{i1} = \vec{v}_i^\dagger A \vec{v}^* = \vec{v}_i^\dagger \sigma \vec{v} = \sigma \delta_{i1},\]
where \(\delta_{ij}\) is the Kronecker delta symbol, and we have used the Takagi property of \(\vec{v}\) and the orthonormality of \(\vec{v}_i^\dagger \vec{v}_j\).
This means only the first component of the first column of \(V^\dagger A V^\ast\) is non-zero, and has value \(\sigma\).
Now since \(A\) is symmetric, we have that
\(\pclosed{V^\dagger A V^\ast}^T = V^\dagger A^T V^\ast = V^\dagger A V^\ast\)
must also be symmetric, and thus must have the form
\[V^\dagger A V^\ast = \begin{pmatrix} \sigma & \vec{0} \\ \vec{0} & A_2 \end{pmatrix},\]
for a symmetric \(A_2 \in M_{n-1}(\mathbb{C})\).


% \ifSubfilesClassLoaded{%
%   \bibliography{references}{}
%   \bibliographystyle{style/JHEP}
% }{}

\end{document}
